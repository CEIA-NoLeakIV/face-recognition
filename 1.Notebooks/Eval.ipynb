{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-Recognition\n",
    "Este notebook tem como objetivo fornecer um meio de avaliar os modelos presentes no Face-Recognition utilizando o dataset LFW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa√ß√µes e Inicializa√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../src/models/face-recognition')\n",
    "from evaluate import eval, compute_metrics_from_predictions, compute_roc_metrics, compute_confusion_matrix\n",
    "from models import (\n",
    "    sphere20,\n",
    "    sphere36,\n",
    "    sphere64,\n",
    "    MobileNetV1,\n",
    "    MobileNetV2,\n",
    "    mobilenet_v3_small,\n",
    "    mobilenet_v3_large\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionar Modelo e Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "1. sphere20\n",
      "2. sphere36\n",
      "3. sphere64\n",
      "4. mobilenetv1\n",
      "5. mobilenetv2\n",
      "6. mobilenetv3_small\n",
      "7. mobilenetv3_large\n",
      "\n",
      "Selected model: mobilenetv3_large\n",
      "Model checkpoint: ../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Modelos Dispon√≠veis\n",
    "available_models = {\n",
    "    \"sphere20\": sphere20,\n",
    "    \"sphere36\": sphere36,\n",
    "    \"sphere64\": sphere64,\n",
    "    \"mobilenetv1\": MobileNetV1,\n",
    "    \"mobilenetv2\": MobileNetV2,\n",
    "    \"mobilenetv3_small\": mobilenet_v3_small,\n",
    "    \"mobilenetv3_large\": mobilenet_v3_large\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, model_name in enumerate(available_models.keys(), 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "\n",
    "# Selecionar peso\n",
    "model_name = \"mobilenetv3_large\"\n",
    "checkpoint_name = \"mobilenetv3_large_5\"\n",
    "embedding_dim = 512\n",
    "model_path = f\"../../src/models/face-recognition/weights/{checkpoint_name}.ckpt\"\n",
    "\n",
    "print(f\"\\nSelected model: {model_name}\")\n",
    "print(f\"Model checkpoint: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete checkpoint from epoch 4\n",
      "Model loaded from: ../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\n",
      "Model ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "model_class = available_models[model_name]\n",
    "model = model_class(embedding_dim=embedding_dim)\n",
    "\n",
    "if model_path and os.path.exists(model_path):\n",
    "    # Carregar checkpoint completo\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Verificar se √© um checkpoint completo ou apenas pesos\n",
    "    if 'model' in checkpoint:\n",
    "        # √â um checkpoint completo - extrair apenas o modelo\n",
    "        model_state_dict = checkpoint['model']\n",
    "        print(f\"Loading complete checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    else:\n",
    "        # S√£o apenas os pesos do modelo\n",
    "        model_state_dict = checkpoint\n",
    "        print(\"Loading model weights only\")\n",
    "    \n",
    "    model.load_state_dict(model_state_dict)\n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {model_path}\")\n",
    "    print(\"Evaluating with random weights...\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar Caminho do Dataset LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFW dataset path: ../../data/raw/lfw\n",
      "‚úì Path exists\n"
     ]
    }
   ],
   "source": [
    "lfw_dataset_path = \"../../data/raw/lfw\"  # Ajuste conforme necess√°rio\n",
    "threshold = 0.35  # Limiar de similaridade para classifica√ß√£o\n",
    "save_metrics_path = \"../../assets/evaluation_metrics\"  # Pasta para salvar gr√°ficos\n",
    "\n",
    "# Criar diret√≥rio para m√©tricas\n",
    "os.makedirs(save_metrics_path, exist_ok=True)\n",
    "\n",
    "print(f\"LFW dataset path: {lfw_dataset_path}\")\n",
    "print(f\"Similarity threshold: {threshold}\")\n",
    "print(f\"Metrics save path: {save_metrics_path}\")\n",
    "\n",
    "if os.path.exists(lfw_dataset_path):\n",
    "    print(\"‚úì Path exists\")\n",
    "else:\n",
    "    print(\"‚úó Warning: Path does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√°lculo das M√©tricas no LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on LFW dataset...\n",
      "==================================================\n",
      "LFW - Avaliacao Simplificada (Somente Pares Positivos):\n",
      "Similaridade Media: 0.6256 | Desvio Padrao: 0.1339\n",
      "==================================================\n",
      "\n",
      "Evaluation complete!\n",
      "Average Similarity Score: 0.6256\n",
      "LFW Accuracy: 0.9650\n",
      "Total pairs evaluated: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting comprehensive evaluation on LFW dataset...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Executar avalia√ß√£o completa com todas as m√©tricas\n",
    "mean_similarity, predictions, metrics = eval(\n",
    "    model,\n",
    "    model_path=None,  # Modelo j√° carregado\n",
    "    lfw_root=lfw_dataset_path,\n",
    "    device=device,\n",
    "    val_dataset='lfw',\n",
    "    compute_full_metrics=True,  # Ativar c√°lculo completo de m√©tricas\n",
    "    save_metrics_path=save_metrics_path,  # Salvar gr√°ficos\n",
    "    threshold=threshold\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Exibir m√©tricas principais\n",
    "print(f\"\\nBasic Metrics:\")\n",
    "print(f\"  Mean Similarity: {metrics['mean_similarity']:.4f} ¬± {metrics['std_similarity']:.4f}\")\n",
    "print(f\"  Precision:       {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:          {metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score:        {metrics['f1']:.4f}\")\n",
    "print(f\"  Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Exibir m√©tricas ROC se dispon√≠veis\n",
    "if 'auc' in metrics:\n",
    "    print(f\"\\nROC Metrics:\")\n",
    "    print(f\"  AUC:             {metrics['auc']:.4f}\")\n",
    "    print(f\"  EER:             {metrics['eer']:.4f} (threshold: {metrics['eer_threshold']:.4f})\")\n",
    "    \n",
    "    # TAR@FAR metrics\n",
    "    for key in metrics:\n",
    "        if key.startswith('TAR@FAR'):\n",
    "            far_value = key.split('=')[1]\n",
    "            print(f\"  TAR@FAR={far_value}: {metrics[key]:.4f}\")\n",
    "\n",
    "# Exibir matriz de confus√£o se dispon√≠vel\n",
    "if 'confusion_matrix' in metrics:\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Negatives:  {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives: {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives: {metrics['false_negatives']}\")\n",
    "    print(f\"  True Positives:  {metrics['true_positives']}\")\n",
    "    print(f\"\\n  FAR (False Accept Rate): {metrics['far']:.4f}\")\n",
    "    print(f\"  FRR (False Reject Rate): {metrics['frr']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total pairs evaluated: {len(predictions)}\")\n",
    "print(f\"Metrics and plots saved to: {save_metrics_path}\")\n",
    "print(f\"  - ROC Curve: lfw_roc_curve.png\")\n",
    "print(f\"  - Confusion Matrix: lfw_confusion_matrix.png\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "if len(predictions) > 0:\n",
    "    from IPython.display import Image, display\n",
    "    \n",
    "    plots_dir = '../../src/models/face-recognition/weights/evaluation_plots'\n",
    "    roc_path = os.path.join(plots_dir, 'lfw_roc_curve.png')\n",
    "    cm_path = os.path.join(plots_dir, 'lfw_confusion_matrix.png')\n",
    "    \n",
    "    print(\"üìà ROC Curve:\")\n",
    "    print(\"=\"*50)\n",
    "    if os.path.exists(roc_path):\n",
    "        display(Image(filename=roc_path))\n",
    "    else:\n",
    "        print(\"ROC curve not found. Make sure save_plots=True in eval().\")\n",
    "    \n",
    "    print(\"\\nüìã Confusion Matrix:\")\n",
    "    print(\"=\"*50)\n",
    "    if os.path.exists(cm_path):\n",
    "        display(Image(filename=cm_path))\n",
    "    else:\n",
    "        print(\"Confusion matrix not found. Make sure save_plots=True in eval().\")\n",
    "else:\n",
    "    print(\"No predictions available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lise em Diferentes Tresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(\"Analyzing threshold sensitivity...\")\n",
    "    \n",
    "    # Testar range de thresholds\n",
    "    test_thresholds = np.linspace(0.1, 0.9, 50)\n",
    "    threshold_metrics = {\n",
    "        'threshold': [],\n",
    "        'accuracy': [],\n",
    "        'f1': [],\n",
    "        'precision': [],\n",
    "        'recall': []\n",
    "    }\n",
    "    \n",
    "    for thresh in test_thresholds:\n",
    "        acc = eval_accuracy(predictions, thresh)\n",
    "        f1 = compute_f1_score(predictions, thresh)\n",
    "        prec = compute_precision(predictions, thresh)\n",
    "        rec = compute_recall(predictions, thresh)\n",
    "        \n",
    "        threshold_metrics['threshold'].append(thresh)\n",
    "        threshold_metrics['accuracy'].append(acc)\n",
    "        threshold_metrics['f1'].append(f1)\n",
    "        threshold_metrics['precision'].append(prec)\n",
    "        threshold_metrics['recall'].append(rec)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['accuracy'], \n",
    "            label='Accuracy', linewidth=2.5, marker='o', markersize=4)\n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['f1'], \n",
    "            label='F1 Score', linewidth=2.5, marker='s', markersize=4)\n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['precision'], \n",
    "            label='Precision', linewidth=2.5, marker='^', markersize=4)\n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['recall'], \n",
    "            label='Recall', linewidth=2.5, marker='v', markersize=4)\n",
    "    \n",
    "    # Marcar best threshold\n",
    "    best_threshold = metrics['best_threshold']\n",
    "    ax.axvline(x=best_threshold, color='red', linestyle='--', \n",
    "              label=f'Best Threshold ({best_threshold:.3f})', linewidth=2.5)\n",
    "    \n",
    "    ax.set_xlabel('Threshold', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Performance Metrics vs Threshold', fontsize=15, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0.1, 0.9])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar\n",
    "    threshold_plot_path = os.path.join(plots_dir, f'{model_name}_threshold_analysis.png')\n",
    "    plt.savefig(threshold_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Threshold analysis saved to: {threshold_plot_path}\")\n",
    "else:\n",
    "    print(\"No predictions available for threshold analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela com Resumo M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Extrair confusion matrix components\n",
    "    cm = metrics['confusion_matrix']\n",
    "    TP = cm[1,1]\n",
    "    FP = cm[0,1]\n",
    "    TN = cm[0,0]\n",
    "    FN = cm[1,0]\n",
    "    \n",
    "    # Calcular derived metrics\n",
    "    TAR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    FAR = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    FRR = FN / (FN + TP) if (FN + TP) > 0 else 0.0\n",
    "    \n",
    "    # Criar tabela\n",
    "    metrics_table = {\n",
    "        'Metric': [\n",
    "            'Mean Similarity',\n",
    "            'Std Similarity',\n",
    "            '---',\n",
    "            'Best Threshold',\n",
    "            'Accuracy',\n",
    "            'F1 Score',\n",
    "            'Precision',\n",
    "            'Recall',\n",
    "            'AUC Score',\n",
    "            '---',\n",
    "            'TAR (True Accept Rate)',\n",
    "            'FAR (False Accept Rate)',\n",
    "            'FRR (False Reject Rate)',\n",
    "            '---',\n",
    "            'True Negatives (TN)',\n",
    "            'False Positives (FP)',\n",
    "            'False Negatives (FN)',\n",
    "            'True Positives (TP)',\n",
    "            '---',\n",
    "            'Total Pairs',\n",
    "            'Positive Pairs'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{metrics['mean_similarity']:.4f}\",\n",
    "            f\"{metrics['std_similarity']:.4f}\",\n",
    "            '',\n",
    "            f\"{metrics['best_threshold']:.4f}\",\n",
    "            f\"{metrics['accuracy']:.4f}\",\n",
    "            f\"{metrics['f1_score']:.4f}\",\n",
    "            f\"{metrics['precision']:.4f}\",\n",
    "            f\"{metrics['recall']:.4f}\",\n",
    "            f\"{metrics['auc_score']:.4f}\",\n",
    "            '',\n",
    "            f\"{TAR:.4f}\",\n",
    "            f\"{FAR:.4f}\",\n",
    "            f\"{FRR:.4f}\",\n",
    "            '',\n",
    "            f\"{TN}\",\n",
    "            f\"{FP}\",\n",
    "            f\"{FN}\",\n",
    "            f\"{TP}\",\n",
    "            '',\n",
    "            f\"{len(predictions)}\",\n",
    "            f\"{int(np.sum(predictions[:, 3].astype(int)))}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_table)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATION SUMMARY - {model_name.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Salvar CSV\n",
    "    csv_path = os.path.join(plots_dir, f'{model_name}_metrics_summary.csv')\n",
    "    df_metrics.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n‚úÖ Metrics summary saved to: {csv_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No predictions available for metrics summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar Json Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exportar Resultados Completos em JSON\n",
    "\n",
    "if len(predictions) > 0:\n",
    "    import json\n",
    "    \n",
    "    # Extrair confusion matrix\n",
    "    cm = metrics['confusion_matrix']\n",
    "    TP, FP, TN, FN = cm[1,1], cm[0,1], cm[0,0], cm[1,0]\n",
    "    \n",
    "    # Calcular derived metrics\n",
    "    TAR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    FAR = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    FRR = FN / (FN + TP) if (FN + TP) > 0 else 0.0\n",
    "    \n",
    "    # Criar estrutura completa\n",
    "    evaluation_results = {\n",
    "        'model_info': {\n",
    "            'model_name': model_name,\n",
    "            'checkpoint': checkpoint_name,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'checkpoint_path': model_path\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'dataset': 'LFW',\n",
    "            'dataset_path': lfw_dataset_path,\n",
    "            'num_pairs': int(len(predictions)),\n",
    "            'num_positive_pairs': int(np.sum(predictions[:, 3].astype(int))),\n",
    "            'num_negative_pairs': int(len(predictions) - np.sum(predictions[:, 3].astype(int)))\n",
    "        },\n",
    "        'similarity_metrics': {\n",
    "            'mean': float(metrics['mean_similarity']),\n",
    "            'std': float(metrics['std_similarity']),\n",
    "            'min': float(np.min(predictions[:, 2].astype(float))),\n",
    "            'max': float(np.max(predictions[:, 2].astype(float))),\n",
    "            'median': float(np.median(predictions[:, 2].astype(float)))\n",
    "        },\n",
    "        'classification_metrics': {\n",
    "            'best_threshold': float(metrics['best_threshold']),\n",
    "            'accuracy': float(metrics['accuracy']),\n",
    "            'f1_score': float(metrics['f1_score']),\n",
    "            'precision': float(metrics['precision']),\n",
    "            'recall': float(metrics['recall']),\n",
    "            'auc_score': float(metrics['auc_score'])\n",
    "        },\n",
    "        'biometric_metrics': {\n",
    "            'TAR': float(TAR),\n",
    "            'FAR': float(FAR),\n",
    "            'FRR': float(FRR)\n",
    "        },\n",
    "        'confusion_matrix': {\n",
    "            'true_negatives': int(TN),\n",
    "            'false_positives': int(FP),\n",
    "            'false_negatives': int(FN),\n",
    "            'true_positives': int(TP)\n",
    "        },\n",
    "        'generated_files': {\n",
    "            'roc_curve': os.path.join(plots_dir, 'lfw_roc_curve.png'),\n",
    "            'confusion_matrix': os.path.join(plots_dir, 'lfw_confusion_matrix.png'),\n",
    "            'threshold_analysis': os.path.join(plots_dir, f'{model_name}_threshold_analysis.png'),\n",
    "            'metrics_csv': os.path.join(plots_dir, f'{model_name}_metrics_summary.csv')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Salvar JSON\n",
    "    json_path = os.path.join(plots_dir, f'{model_name}_evaluation_results.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìÅ EVALUATION FILES GENERATED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ JSON Results:      {json_path}\")\n",
    "    print(f\"‚úÖ CSV Summary:       {csv_path}\")\n",
    "    print(f\"‚úÖ ROC Curve:         {roc_path}\")\n",
    "    print(f\"‚úÖ Confusion Matrix:  {cm_path}\")\n",
    "    print(f\"‚úÖ Threshold Plot:    {threshold_plot_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚úÖ All evaluation metrics calculated and exported successfully!\")\n",
    "else:\n",
    "    print(\"No predictions available for export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lise do Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Statistics:\n",
      "Mean: 0.6256\n",
      "Std: 0.1339\n",
      "Min: 0.1134\n",
      "Max: 0.9901\n",
      "Median: 0.6407\n"
     ]
    }
   ],
   "source": [
    "if len(predictions) > 0:\n",
    "    similarities = predictions[:, 2].astype(float)\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    print(\"Similarity Statistics:\")\n",
    "    print(f\"Mean: {np.mean(similarities):.4f}\")\n",
    "    print(f\"Std: {np.std(similarities):.4f}\")\n",
    "    print(f\"Min: {np.min(similarities):.4f}\")\n",
    "    print(f\"Max: {np.max(similarities):.4f}\")\n",
    "    print(f\"Median: {np.median(similarities):.4f}\")\n",
    "else:\n",
    "    print(\"No predictions to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar V√°rios Modelos (Batch Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de modelos para avaliar\n",
    "models_to_evaluate = [\n",
    "    (\"mobilenetv3_large\", \"../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\"),\n",
    "    # Adicione mais modelos aqui\n",
    "    # (\"sphere20\", \"../../src/models/face-recognition/weights/sphere20_mcp.ckpt\"),\n",
    "    # (\"mobilenetv2\", \"../../src/models/face-recognition/weights/mobilenetv2_mcp.ckpt\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"\\nBatch Evaluation of Multiple Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, checkpoint_path in models_to_evaluate:\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Skipping {model_name}: checkpoint not found at {checkpoint_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    model_class = available_models[model_name]\n",
    "    model_eval = model_class(embedding_dim=512).to(device)\n",
    "    \n",
    "    # Carregar checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    if 'model' in checkpoint:\n",
    "        model_eval.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model_eval.load_state_dict(checkpoint)\n",
    "    \n",
    "    # Avaliar\n",
    "    score, preds, m = eval(\n",
    "        model_eval,\n",
    "        model_path=None,\n",
    "        lfw_root=lfw_dataset_path,\n",
    "        device=device,\n",
    "        compute_full_metrics=False,  # Mais r√°pido para compara√ß√£o\n",
    "        threshold=threshold\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'mean_similarity': score,\n",
    "        'precision': m['precision'],\n",
    "        'recall': m['recall'],\n",
    "        'f1': m['f1'],\n",
    "        'accuracy': m['accuracy'],\n",
    "        'num_pairs': len(preds)\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Model':<20} {'Similarity':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Accuracy':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['model']:<20} {result['mean_similarity']:<12.4f} \"\n",
    "              f\"{result['precision']:<12.4f} {result['recall']:<12.4f} \"\n",
    "              f\"{result['f1']:<12.4f} {result['accuracy']:<12.4f}\")\n",
    "else:\n",
    "    print(\"No models evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspe√ß√£o de Predi√ß√£o de Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Mostrar alguns pares positivos\n",
    "    positive_pairs = predictions[predictions[:, 3] == '1']\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(\"POSITIVE PAIRS (Same Person) - First 3:\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    for i in range(min(3, len(positive_pairs))):\n",
    "        path1, path2, similarity, gt = positive_pairs[i]\n",
    "        print(f\"\\nPair {i+1}:\")\n",
    "        print(f\"  Image 1:    {os.path.basename(path1)}\")\n",
    "        print(f\"  Image 2:    {os.path.basename(path2)}\")\n",
    "        print(f\"  Similarity: {float(similarity):.4f}\")\n",
    "        print(f\"  Prediction: {'‚úì SAME' if float(similarity) > threshold else '‚úó DIFFERENT'}\")\n",
    "    \n",
    "    # Mostrar alguns pares negativos se existirem\n",
    "    negative_pairs = predictions[predictions[:, 3] == '0']\n",
    "    if len(negative_pairs) > 0:\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(\"NEGATIVE PAIRS (Different Person) - First 3:\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        for i in range(min(3, len(negative_pairs))):\n",
    "            path1, path2, similarity, gt = negative_pairs[i]\n",
    "            print(f\"\\nPair {i+1}:\")\n",
    "            print(f\"  Image 1:    {os.path.basename(path1)}\")\n",
    "            print(f\"  Image 2:    {os.path.basename(path2)}\")\n",
    "            print(f\"  Similarity: {float(similarity):.4f}\")\n",
    "            print(f\"  Prediction: {'‚úó SAME' if float(similarity) > threshold else '‚úì DIFFERENT'}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "else:\n",
    "    print(\"No predictions to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
