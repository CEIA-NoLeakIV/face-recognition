{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-Recognition\n",
    "Este notebook tem como objetivo fornecer um meio de avaliar os modelos presentes no Face-Recognition utilizando o dataset LFW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImportaÃ§Ãµes e InicializaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../../src/models/face-recognition')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from evaluate import (\n",
    "    eval_accuracy,\n",
    "    find_best_threshold,\n",
    "    compute_roc_curve,\n",
    "    compute_f1_score,\n",
    "    compute_precision,\n",
    "    compute_recall,\n",
    "    compute_confusion_matrix\n",
    ")\n",
    "\n",
    "from models import (\n",
    "    sphere20,\n",
    "    sphere36,\n",
    "    sphere64,\n",
    "    MobileNetV1,\n",
    "    MobileNetV2,\n",
    "    mobilenet_v3_small,\n",
    "    mobilenet_v3_large\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionar Modelo e Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "1. sphere20\n",
      "2. sphere36\n",
      "3. sphere64\n",
      "4. mobilenetv1\n",
      "5. mobilenetv2\n",
      "6. mobilenetv3_small\n",
      "7. mobilenetv3_large\n",
      "\n",
      "Selected model: mobilenetv3_large\n",
      "Model checkpoint: ../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Modelos DisponÃ­veis\n",
    "available_models = {\n",
    "    \"sphere20\": sphere20,\n",
    "    \"sphere36\": sphere36,\n",
    "    \"sphere64\": sphere64,\n",
    "    \"mobilenetv1\": MobileNetV1,\n",
    "    \"mobilenetv2\": MobileNetV2,\n",
    "    \"mobilenetv3_small\": mobilenet_v3_small,\n",
    "    \"mobilenetv3_large\": mobilenet_v3_large\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for i, model_name in enumerate(available_models.keys(), 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "\n",
    "# Selecionar peso\n",
    "model_name = \"mobilenetv3_large\"\n",
    "checkpoint_name = \"mobilenetv3_large_5\"\n",
    "embedding_dim = 512\n",
    "model_path = f\"../../src/models/face-recognition/weights/{checkpoint_name}.ckpt\"\n",
    "\n",
    "print(f\"\\nSelected model: {model_name}\")\n",
    "print(f\"Model checkpoint: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete checkpoint from epoch 4\n",
      "Model loaded from: ../../src/models/face-recognition/weights/mobilenetv3_large_5.ckpt\n",
      "Model ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "model_class = available_models[model_name]\n",
    "model = model_class(embedding_dim=embedding_dim)\n",
    "\n",
    "if model_path and os.path.exists(model_path):\n",
    "    # Carregar checkpoint completo\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Verificar se Ã© um checkpoint completo ou apenas pesos\n",
    "    if 'model' in checkpoint:\n",
    "        # Ã‰ um checkpoint completo - extrair apenas o modelo\n",
    "        model_state_dict = checkpoint['model']\n",
    "        print(f\"Loading complete checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    else:\n",
    "        # SÃ£o apenas os pesos do modelo\n",
    "        model_state_dict = checkpoint\n",
    "        print(\"Loading model weights only\")\n",
    "    \n",
    "    model.load_state_dict(model_state_dict)\n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {model_path}\")\n",
    "    print(\"Evaluating with random weights...\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar Caminho do Dataset LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFW dataset path: ../../data/raw/lfw\n",
      "âœ“ Path exists\n"
     ]
    }
   ],
   "source": [
    "lfw_dataset_path = \"../../data/raw/lfw\"  # Adjuste conforme necessÃ¡rio\n",
    "\n",
    "print(f\"LFW dataset path: {lfw_dataset_path}\")\n",
    "if os.path.exists(lfw_dataset_path):\n",
    "    print(\"âœ“ Path exists\")\n",
    "else:\n",
    "    print(\"âœ— Warning: Path does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CÃ¡lculo das MÃ©tricas no LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on LFW dataset...\n",
      "==================================================\n",
      "LFW - Avaliacao Simplificada (Somente Pares Positivos):\n",
      "Similaridade Media: 0.6256 | Desvio Padrao: 0.1339\n",
      "==================================================\n",
      "\n",
      "Evaluation complete!\n",
      "Average Similarity Score: 0.6256\n",
      "LFW Accuracy: 0.9650\n",
      "Total pairs evaluated: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting evaluation on LFW dataset...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Executar avaliaÃ§Ã£o completa com todas as novas mÃ©tricas\n",
    "similarity_score, predictions, metrics = eval(\n",
    "    model, \n",
    "    model_path=None, \n",
    "    val_dataset='lfw',\n",
    "    val_root=lfw_dataset_path, \n",
    "    device=device,\n",
    "    compute_metrics=True,  # Ativa cÃ¡lculo de todas as mÃ©tricas\n",
    "    save_plots=True,       # Salva grÃ¡ficos ROC e Confusion Matrix\n",
    "    plots_dir='../../src/models/face-recognition/weights/evaluation_plots'\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Exibir resultados completos\n",
    "if len(predictions) > 0:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # MÃ©tricas de Similaridade\n",
    "    print(f\"\\nðŸ“Š Similarity Metrics:\")\n",
    "    print(f\"  Mean Similarity:     {metrics['mean_similarity']:.4f}\")\n",
    "    print(f\"  Std Similarity:      {metrics['std_similarity']:.4f}\")\n",
    "    \n",
    "    # MÃ©tricas de ClassificaÃ§Ã£o\n",
    "    print(f\"\\nðŸŽ¯ Classification Metrics (Threshold: {metrics['best_threshold']:.4f}):\")\n",
    "    print(f\"  Accuracy:            {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Score:            {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Precision:           {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:              {metrics['recall']:.4f}\")\n",
    "    print(f\"  AUC Score:           {metrics['auc_score']:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nðŸ“‹ Confusion Matrix:\")\n",
    "    print(f\"                    Predicted\")\n",
    "    print(f\"                 Different    Same\")\n",
    "    print(f\"  Actual Different   {cm[0,0]:5d}    {cm[0,1]:5d}  (TN, FP)\")\n",
    "    print(f\"         Same        {cm[1,0]:5d}    {cm[1,1]:5d}  (FN, TP)\")\n",
    "    \n",
    "    # Derived Metrics\n",
    "    TP = cm[1,1]\n",
    "    FP = cm[0,1]\n",
    "    TN = cm[0,0]\n",
    "    FN = cm[1,0]\n",
    "    \n",
    "    TAR = TP / (TP + FN) if (TP + FN) > 0 else 0.0  # True Acceptance Rate (= Recall)\n",
    "    FAR = FP / (FP + TN) if (FP + TN) > 0 else 0.0  # False Acceptance Rate\n",
    "    FRR = FN / (FN + TP) if (FN + TP) > 0 else 0.0  # False Rejection Rate\n",
    "    \n",
    "    print(f\"\\nðŸ” Biometric Security Metrics:\")\n",
    "    print(f\"  TAR (True Acceptance Rate):  {TAR:.4f}\")\n",
    "    print(f\"  FAR (False Acceptance Rate): {FAR:.4f}\")\n",
    "    print(f\"  FRR (False Rejection Rate):  {FRR:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Dataset Info:\")\n",
    "    print(f\"  Total pairs evaluated: {len(predictions)}\")\n",
    "    print(f\"  Positive pairs:        {int(np.sum(predictions[:, 3].astype(int)))}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Generated Files:\")\n",
    "    plots_dir = '../../src/models/face-recognition/weights/evaluation_plots'\n",
    "    print(f\"  ROC Curve:        {plots_dir}/lfw_roc_curve.png\")\n",
    "    print(f\"  Confusion Matrix: {plots_dir}/lfw_confusion_matrix.png\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"Average Similarity Score: {similarity_score:.4f}\")\n",
    "    print(f\"No valid pairs found for metrics calculation\")\n",
    "    print(f\"Total pairs evaluated: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualizaÃ§Ã£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "if len(predictions) > 0:\n",
    "    from IPython.display import Image, display\n",
    "    \n",
    "    plots_dir = '../../src/models/face-recognition/weights/evaluation_plots'\n",
    "    roc_path = os.path.join(plots_dir, 'lfw_roc_curve.png')\n",
    "    cm_path = os.path.join(plots_dir, 'lfw_confusion_matrix.png')\n",
    "    \n",
    "    print(\"ðŸ“ˆ ROC Curve:\")\n",
    "    print(\"=\"*50)\n",
    "    if os.path.exists(roc_path):\n",
    "        display(Image(filename=roc_path))\n",
    "    else:\n",
    "        print(\"ROC curve not found. Make sure save_plots=True in eval().\")\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Confusion Matrix:\")\n",
    "    print(\"=\"*50)\n",
    "    if os.path.exists(cm_path):\n",
    "        display(Image(filename=cm_path))\n",
    "    else:\n",
    "        print(\"Confusion matrix not found. Make sure save_plots=True in eval().\")\n",
    "else:\n",
    "    print(\"No predictions available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnÃ¡lise em Diferentes Tresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(\"Analyzing threshold sensitivity...\")\n",
    "    \n",
    "    # Testar range de thresholds\n",
    "    test_thresholds = np.linspace(0.1, 0.9, 50)\n",
    "    threshold_metrics = {\n",
    "        'threshold': [],\n",
    "        'accuracy': [],\n",
    "        'f1': [],\n",
    "        'precision': [],\n",
    "        'recall': []\n",
    "    }\n",
    "    \n",
    "    for thresh in test_thresholds:\n",
    "        acc = eval_accuracy(predictions, thresh)\n",
    "        f1 = compute_f1_score(predictions, thresh)\n",
    "        prec = compute_precision(predictions, thresh)\n",
    "        rec = compute_recall(predictions, thresh)\n",
    "        \n",
    "        threshold_metrics['threshold'].append(thresh)\n",
    "        threshold_metrics['accuracy'].append(acc)\n",
    "        threshold_metrics['f1'].append(f1)\n",
    "        threshold_metrics['precision'].append(prec)\n",
    "        threshold_metrics['recall'].append(rec)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['accuracy'], \n",
    "            label='Accuracy', linewidth=2.5, marker='o', markersize=4)\n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['f1'], \n",
    "            label='F1 Score', linewidth=2.5, marker='s', markersize=4)\n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['precision'], \n",
    "            label='Precision', linewidth=2.5, marker='^', markersize=4)\n",
    "    ax.plot(threshold_metrics['threshold'], threshold_metrics['recall'], \n",
    "            label='Recall', linewidth=2.5, marker='v', markersize=4)\n",
    "    \n",
    "    # Marcar best threshold\n",
    "    best_threshold = metrics['best_threshold']\n",
    "    ax.axvline(x=best_threshold, color='red', linestyle='--', \n",
    "              label=f'Best Threshold ({best_threshold:.3f})', linewidth=2.5)\n",
    "    \n",
    "    ax.set_xlabel('Threshold', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Performance Metrics vs Threshold', fontsize=15, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0.1, 0.9])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar\n",
    "    threshold_plot_path = os.path.join(plots_dir, f'{model_name}_threshold_analysis.png')\n",
    "    plt.savefig(threshold_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Threshold analysis saved to: {threshold_plot_path}\")\n",
    "else:\n",
    "    print(\"No predictions available for threshold analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela com Resumo MÃ©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Extrair confusion matrix components\n",
    "    cm = metrics['confusion_matrix']\n",
    "    TP = cm[1,1]\n",
    "    FP = cm[0,1]\n",
    "    TN = cm[0,0]\n",
    "    FN = cm[1,0]\n",
    "    \n",
    "    # Calcular derived metrics\n",
    "    TAR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    FAR = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    FRR = FN / (FN + TP) if (FN + TP) > 0 else 0.0\n",
    "    \n",
    "    # Criar tabela\n",
    "    metrics_table = {\n",
    "        'Metric': [\n",
    "            'Mean Similarity',\n",
    "            'Std Similarity',\n",
    "            '---',\n",
    "            'Best Threshold',\n",
    "            'Accuracy',\n",
    "            'F1 Score',\n",
    "            'Precision',\n",
    "            'Recall',\n",
    "            'AUC Score',\n",
    "            '---',\n",
    "            'TAR (True Accept Rate)',\n",
    "            'FAR (False Accept Rate)',\n",
    "            'FRR (False Reject Rate)',\n",
    "            '---',\n",
    "            'True Negatives (TN)',\n",
    "            'False Positives (FP)',\n",
    "            'False Negatives (FN)',\n",
    "            'True Positives (TP)',\n",
    "            '---',\n",
    "            'Total Pairs',\n",
    "            'Positive Pairs'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{metrics['mean_similarity']:.4f}\",\n",
    "            f\"{metrics['std_similarity']:.4f}\",\n",
    "            '',\n",
    "            f\"{metrics['best_threshold']:.4f}\",\n",
    "            f\"{metrics['accuracy']:.4f}\",\n",
    "            f\"{metrics['f1_score']:.4f}\",\n",
    "            f\"{metrics['precision']:.4f}\",\n",
    "            f\"{metrics['recall']:.4f}\",\n",
    "            f\"{metrics['auc_score']:.4f}\",\n",
    "            '',\n",
    "            f\"{TAR:.4f}\",\n",
    "            f\"{FAR:.4f}\",\n",
    "            f\"{FRR:.4f}\",\n",
    "            '',\n",
    "            f\"{TN}\",\n",
    "            f\"{FP}\",\n",
    "            f\"{FN}\",\n",
    "            f\"{TP}\",\n",
    "            '',\n",
    "            f\"{len(predictions)}\",\n",
    "            f\"{int(np.sum(predictions[:, 3].astype(int)))}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_table)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATION SUMMARY - {model_name.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Salvar CSV\n",
    "    csv_path = os.path.join(plots_dir, f'{model_name}_metrics_summary.csv')\n",
    "    df_metrics.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nâœ… Metrics summary saved to: {csv_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No predictions available for metrics summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar Json Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exportar Resultados Completos em JSON\n",
    "\n",
    "if len(predictions) > 0:\n",
    "    import json\n",
    "    \n",
    "    # Extrair confusion matrix\n",
    "    cm = metrics['confusion_matrix']\n",
    "    TP, FP, TN, FN = cm[1,1], cm[0,1], cm[0,0], cm[1,0]\n",
    "    \n",
    "    # Calcular derived metrics\n",
    "    TAR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    FAR = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    FRR = FN / (FN + TP) if (FN + TP) > 0 else 0.0\n",
    "    \n",
    "    # Criar estrutura completa\n",
    "    evaluation_results = {\n",
    "        'model_info': {\n",
    "            'model_name': model_name,\n",
    "            'checkpoint': checkpoint_name,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'checkpoint_path': model_path\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'dataset': 'LFW',\n",
    "            'dataset_path': lfw_dataset_path,\n",
    "            'num_pairs': int(len(predictions)),\n",
    "            'num_positive_pairs': int(np.sum(predictions[:, 3].astype(int))),\n",
    "            'num_negative_pairs': int(len(predictions) - np.sum(predictions[:, 3].astype(int)))\n",
    "        },\n",
    "        'similarity_metrics': {\n",
    "            'mean': float(metrics['mean_similarity']),\n",
    "            'std': float(metrics['std_similarity']),\n",
    "            'min': float(np.min(predictions[:, 2].astype(float))),\n",
    "            'max': float(np.max(predictions[:, 2].astype(float))),\n",
    "            'median': float(np.median(predictions[:, 2].astype(float)))\n",
    "        },\n",
    "        'classification_metrics': {\n",
    "            'best_threshold': float(metrics['best_threshold']),\n",
    "            'accuracy': float(metrics['accuracy']),\n",
    "            'f1_score': float(metrics['f1_score']),\n",
    "            'precision': float(metrics['precision']),\n",
    "            'recall': float(metrics['recall']),\n",
    "            'auc_score': float(metrics['auc_score'])\n",
    "        },\n",
    "        'biometric_metrics': {\n",
    "            'TAR': float(TAR),\n",
    "            'FAR': float(FAR),\n",
    "            'FRR': float(FRR)\n",
    "        },\n",
    "        'confusion_matrix': {\n",
    "            'true_negatives': int(TN),\n",
    "            'false_positives': int(FP),\n",
    "            'false_negatives': int(FN),\n",
    "            'true_positives': int(TP)\n",
    "        },\n",
    "        'generated_files': {\n",
    "            'roc_curve': os.path.join(plots_dir, 'lfw_roc_curve.png'),\n",
    "            'confusion_matrix': os.path.join(plots_dir, 'lfw_confusion_matrix.png'),\n",
    "            'threshold_analysis': os.path.join(plots_dir, f'{model_name}_threshold_analysis.png'),\n",
    "            'metrics_csv': os.path.join(plots_dir, f'{model_name}_metrics_summary.csv')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Salvar JSON\n",
    "    json_path = os.path.join(plots_dir, f'{model_name}_evaluation_results.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“ EVALUATION FILES GENERATED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"âœ… JSON Results:      {json_path}\")\n",
    "    print(f\"âœ… CSV Summary:       {csv_path}\")\n",
    "    print(f\"âœ… ROC Curve:         {roc_path}\")\n",
    "    print(f\"âœ… Confusion Matrix:  {cm_path}\")\n",
    "    print(f\"âœ… Threshold Plot:    {threshold_plot_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nâœ… All evaluation metrics calculated and exported successfully!\")\n",
    "else:\n",
    "    print(\"No predictions available for export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnÃ¡lise do Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Statistics:\n",
      "Mean: 0.6256\n",
      "Std: 0.1339\n",
      "Min: 0.1134\n",
      "Max: 0.9901\n",
      "Median: 0.6407\n"
     ]
    }
   ],
   "source": [
    "if len(predictions) > 0:\n",
    "    similarities = predictions[:, 2].astype(float)\n",
    "    \n",
    "    # EstatÃ­sticas\n",
    "    print(\"Similarity Statistics:\")\n",
    "    print(f\"Mean: {np.mean(similarities):.4f}\")\n",
    "    print(f\"Std: {np.std(similarities):.4f}\")\n",
    "    print(f\"Min: {np.min(similarities):.4f}\")\n",
    "    print(f\"Max: {np.max(similarities):.4f}\")\n",
    "    print(f\"Median: {np.median(similarities):.4f}\")\n",
    "else:\n",
    "    print(\"No predictions to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar VÃ¡rios Modelos (Batch Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [\n",
    "    (\"sphere20\", \"../../src/models/face-recognition/weights/sphere20_mcp.ckpt\"),\n",
    "    (\"mobilenetv1\", \"../../src/models/face-recognition/weights/mobilenetv1_mcp.ckpt\"),\n",
    "    (\"mobilenetv2\", \"../../src/models/face-recognition/weights/mobilenetv2_mcp.ckpt\"),\n",
    "    # Adicione mais modelos conforme necessÃ¡rio\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"Batch Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, checkpoint_path in models_to_evaluate:\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Skipping {model_name}: checkpoint not found at {checkpoint_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    model_class = available_models[model_name]\n",
    "    model = model_class(embedding_dim=512).to(device)\n",
    "    \n",
    "    # CORREÃ‡ÃƒO: Passar o checkpoint_path para carregar o modelo treinado\n",
    "    score, preds = eval(model, model_path=checkpoint_path, lfw_root=lfw_dataset_path, device=device)\n",
    "    \n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'score': score,\n",
    "        'num_pairs': len(preds)\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result['model']:20s} | Score: {result['score']:.4f} | Pairs: {result['num_pairs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InspeÃ§Ã£o de PrediÃ§Ã£o de Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions) > 0:\n",
    "    print(\"Sample Predictions (first 5):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i in range(min(5, len(predictions))):\n",
    "        path1, path2, similarity, gt = predictions[i]\n",
    "        print(f\"\\nPair {i+1}:\")\n",
    "        print(f\"  Image 1: {path1}\")\n",
    "        print(f\"  Image 2: {path2}\")\n",
    "        print(f\"  Similarity: {float(similarity):.4f}\")\n",
    "        print(f\"  Ground Truth: {'Same' if gt == '1' else 'Different'}\")\n",
    "else:\n",
    "    print(\"No predictions to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
